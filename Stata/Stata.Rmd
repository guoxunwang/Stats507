---
title: "Stats506 F18, Group Project, Stata"
author: "Xun Wang, xunwang@umich.edu"
date: "Novmber 22,2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# 80: --------------------------------------------------------------------------
```

#Tutorial to Ridge Regression
##Load, Mung and Summarize Data
First, we load data into Stata and we could see the dimension of this dataset
when we import it.
```{r eval=FALSE}
import delimited meatspec.csv,clear
```
![](import.PNG){width=37%}

Then we use summarize command to see the brief description of this dataset.
Because our dataset has 100 independent variables and 1 dependent variable.
We only show the first six of them.
```{r eval=FALSE}
summarize v1 v2 v3 v4 v5 fat
```
![](summary.PNG){width=72%}


##Ridge Regression using Package: ridgereg
###Function Description
Because package:ridgereg is not included in Stata, we need to install it first.
We will use several options of this function ridgereg: 
model(orr): ordinary ridge regression.

kr(): ridge $\lambda$ value, it is must be in the range (0 < k < 1).

coll: keep collinear variables; default is removing collinear variables.

diag: Model Selection Diagnostic Criteria; this option returns values of many
functions, for example, Log Likelihood Function(LLF) and Akaike Information 
Criterion(AIC). We will use Craven-Wahba Generalized Cross Validation(GCV) of
the regression result for this case.

See references for more information about this package.
```{r eval=FALSE}
ssc install ridgereg
```

###Standardization
As recommanded by the method of ridge regression, we need to standarized our 
predictors and centered the response first. Using for loop to achieve this 
object:
```{r eval=FALSE}
summarize fat
local mean_fat=r(mean)
replace fat=fat-r(mean) 
foreach i of numlist 1/100 {
    summarize v`i'
	replace v`i'=(v`i'-r(mean))/r(sd)
}
```

###$\beta$ridge
As we know from introsuction, the coefficients of ridge regression shrink 
to 0 with the increasing of $\lambda$ value. If we visualize this process in Stata,
we first do ridge regression on each $\lambda$ and then plot coefficients against
$\lambda$ value. We could get a plot as below:
```{r eval=FALSE}
preserve
ridgereg fat v* if mod(n,5)!=0,model(orr) kr(0) coll diag
matrix coeff=e(b)
local index=1
generate lambda=0
foreach i of numlist 1e-9(1e-9)5e-8{
    local index=`index'+1
    ridgereg fat v* if mod(n,5)!=0,model(orr) kr(`i') coll diag
	replace lambda=`i' in `index'
	matrix b=e(b)
	matrix coeff=(coeff\b)
}
svmat coeff,names(coeff)
line coeff1-coeff99 lambda
restore
```

![](coeff.PNG){width=72%}

As we could see, all coefficients converge to 0 finally.

###Selection of $\lambda$
Often there is insufficient data for us to do model assessment. Cross-Validation 
comes to be a commom solutions for this problem based on predictions. In this 
way, we set some data aside for test and use the other part do training-validation
step. During this process, we choose a series of tuning parameter and do regression
on various $\lambda$ values to compare generalized cross-validation estimates.
After that, we choose a model which has lowest GCV value and test its performance
on prediction.

####Group
First, we generate a column of case numbers for this dataset.
```{r eval=FALSE}
generate n=_n
```

In this case, we split our dataset into 5 groups. One of the group is test data, 
whose case number could be divided by 5 and without reminder. Others are 
training and validation data.

####Cross-Validation
In this step, we could simply do training and validation process together based 
on a model selection diagnostic criteria returned by Stata itself. So, we could 
save much time. This criteria is generalized cross-validation(GCV) value and it 
is in ereturn list after doing ridge regression. We just need to choose a 
$\lambda$ which optimizes this value. 

The porcess is below. Doing this loop 100 times for different $\lambda$ values 
and generating a list to save GCV value and corresponding $\lambda$.
```{r eval=FALSE}
generate gcv=10
generate lambda=0
local index=0
foreach i of numlist 0(1e-11)1e-9{
    local index=`index'+1
    ridgereg fat v* if mod(n,5)!=0,model(orr) kr(`i') coll diag
	replace lambda=`i' in `index'
	replace gcv=e(gcv) in `index'
}
```

Then we could plot GCV values against $\lambda$. Soomthing this line.
```{r eval=FALSE}
lowess gcv lambda if n<=101
```
![](plot.PNG){width=47%}


From the plot above, we could see that GCV estimates reaches its minimum around
5e-10. It is the same with the result computed below.
```{r eval=FALSE}
summarize gcv if n<=101
summarize lambda if gcv==r(min)
```
![](gcv.PNG){width=67%}

![](lambda.PNG){width=67%}

So, we could conclude that $\lambda=5e-10$ for this case.

####Test model
When come to the last step of model assessment, we try to test the performance 
of our exisiting model on the test data we keep at first.
So, we fit the same model on the same dataset by assigning $\lambda$ and then 
use matrix manipulation to get predictions.
Don't forget that fat variable is centered at first. We need to add back mean 
value at last to get preditions of fat on all groups of observations.
```{r eval=FALSE}
ridgereg fat v* if mod(n,5)!=0,model(orr) kr(5e-10) coll predict(pred)
matrix coeff=e(b)'
generate constant=1
mkmat v* constant, matrix(predictors)
matrix pred_fat=predictors*coeff
svmat pred_fat,names(pred_fat)
```

Finally, MSE of predictions could be computed .Review the formula in introduction to see how it works. Here is the way we could computed in Stata.
```{r eval=FALSE}
generate err_fat=(pred_fat-fat)^2
summarize err_fat if mod(n,5)==0
display r(mean)
```
![](performance.PNG){width=67%}

We could see that the MSE of one point even reaches 727. Actually it is an 
abnormal value. If we exclude this observation, the MSE is much better.

![](exclude.PNG){width=67%}

If we use this result to predict, RMSE(Root Mean Sqaured Error), which is the 
root of MSE, is to be used. RMSE equals 2.377 and it is relatively good.

##Conclusion
As we could see from above, GCV values against $\lambda$ is not continuous. This
may bacause predictors are highly correlated and some predictors could not provide extra information for explaining response, so their coefficients are forced to 0.
And their coefficients, which equal 0, are not totally the same with the increasing
of $\lambda$. In this case, GCV values will have a relative large change compared
to last ones. As a result, the graph of GCV values is not strictly continuous.
In addition, $\lambda$ is very small after standardization. 

##References
**ridgereg package** http://www.haghish.com/statistics/stata-blog/stata-programming/download/ridgereg.html

Faraway,Julian James. *Linear Models with R*. CRC Press LLC, 2009.

